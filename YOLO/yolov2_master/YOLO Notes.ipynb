{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO short Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO works by dividing the input images into grids of S x S (This is done through convolution layers that subsamples the original input image down to a factor of M. Therefore, input size/M = S) and placing anchors (predetermined boxes) in each grid cell. The coordinate of the bounding boxes (x1,y1,x2,y2) will be converted to (center x, center y, width and height) format. However, it's not a naive straightforward conversion. \n",
    "\n",
    "The **center** of every ground truth bounding box has to fall in one of the grids. That particular grid will be known as the **responsible grid**. The anchor with the **highest** IoU with the ground truth bounding box will be selected. This particular anchor is known as the **responsible anchor**. Every object in image will be assigned with a responsible anchor. The bounding box converted coordinate will be based on this responsible anchor. \n",
    "\n",
    "**Center X and Center Y** : These are the values of the anchor's offset from the original center of the ground-truth box.\n",
    "\n",
    "**Width and Height** : These are the values of the anchor's width and height's offset from the ground-truth box.\n",
    "\n",
    "**NOTE** : Refer to the original paper for the exact formula for the conversion.\n",
    "\n",
    "This (center x, center y, width, height) will be the values that the neural network will be try to predict. In other words, for every anchor, the neural network will produce a prediction. There will also be predictions on the confidence value and the class of the object. The confidence value is what we'll be using to filter out unwanted predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load ImageNet dataset and train the network for 160 epochs.\n",
    "\n",
    "2) The \"learned\" weights from the convolutional layers will be loaded into YOLO model.\n",
    "\n",
    "3) Calculate the best sizes for the anchors using K-Means on the training data.\n",
    "\n",
    "4) Based on the anchors, calculate the values of (center x, center y, width, height) for every image. \n",
    "\n",
    "5) Save the anchor values for the specific image size in Python DBM.\n",
    "\n",
    "6) Convert both the image arrays and the label arrays into Torch Tensor.\n",
    "\n",
    "7) Train YOLO's network.\n",
    "   a) For every 10 epoch, a randomly chosen different image size will be used.\n",
    "   b) Learning rate will be reset after every 200 epoch.\n",
    "   \n",
    "8) Output from the network will be filtered through Non-Max Suppression.\n",
    "\n",
    "9) mAP will be calculated.\n",
    "\n",
    "10) For every 10 epoch, model will be saved based on the mAP score.\n",
    "\n",
    "11) The data from the Python DBM will be written to a file. It contains the anchor sizes used during the training for each image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load the images from the test folder into numpy array and convert them into Torch Tensor.\n",
    "\n",
    "2) Load the anchor sizes for the given image size from the saved file during training.\n",
    "\n",
    "3) Load the saved YOLO weights. \n",
    "\n",
    "4) Predict outputs and filter them through NMS.\n",
    "\n",
    "5) Draw the bounding boxes on the images and save them into the output folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
